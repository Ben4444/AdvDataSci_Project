---
title: "Advanced Data Science I Project"
author: "Ben Barrett"
date: "October 8, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
**Introduction**

These methods are designed to pull Twitter data to identify times and areas hardest hit by Hurricane Harvey. Geocoding the tweets primarily relies on the user location (entered once when a user first starts Twitter), rather than the location associated with an individual tweet, as most users turn the tweet location data off. In this case, however, these methods should be appropriate - as people who fled the impacted area before the storm hit will still have their user location linked to the impacted area. This mode of analysis relies on the assumption that the number of tweets about Hurricane Harvey coming
out of an area correlates with the level of destruction in that area.


**Scraping Data**

In order to use pull tweets about Hurricane Harvey off of Twitter, I first had to gain access to the Twitter API, which I completed following the referenced online tutorial (1). Here is the link to my [Twitter app page](https://apps.twitter.com/app/14193250).

Next, I needed to download and install the Python library 'Tweepy' in order to connect to the Twitter Streaming API and download data. I have done this by following these insllation instructions (2). It should be noted that Python and pip must be installed before Tweepy can be downloaded and installed. I next created a Python program in Notepad++ called twitter_streaming.py, based off of the referenced tutorial (3). I started the stream by searching for the hashtags 'HurricaneHarvey' and 'HurricaneHarveyRelief'. The output is being exported to a JSON file, twitter_data.json, and is available in my Dropbox.

I began the Twitter data stream at 9:10AM on 9/1/2017, and stopped the Twitter data stream at 9:56AM on 9/4/2017. The program run time was 36 hours, 46 minutes. A total of 3,289,336 KB (3.290 GB) of Twitter data was collected, which corresponds to 1,491.086 KB of data per minute, on average.

```{r engine='python', highlight=TRUE, eval=FALSE}
#twitter_streaming.py

#Import the necessary methods from tweepy library
import tweepy
from tweepy import Stream
from tweepy.streaming import StreamListener 
from tweepy import OAuthHandler
import json

#Variables that contains the user credentials to access Twitter API 
access_token = "Access_Token"
access_token_secret = "Access_Token_Secret"
consumer_key = "Consumer_Key"
consumer_secret = "Consumer_Secret"

auth = OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
 
api = tweepy.API(auth)
@classmethod
def parse(cls, api, raw):
    status = cls.first_parse(api, raw)
    setattr(status, 'json', json.dumps(raw))
    return status
 
# Status() is the data model for a tweet
tweepy.models.Status.first_parse = tweepy.models.Status.parse
tweepy.models.Status.parse = parse
class MyListener(StreamListener):

     def on_data(self, data):
        try:
            with open('twitter_data.json', 'a') as f:
                f.write(data)
                return True
        except BaseException as e:
            print("Error on_data: %s" % str(e))
        return True
 
     def on_error(self, status):
        print(status)
        return True
 
#Set the hashtag to be searched
twitter_stream = Stream(auth, MyListener())
twitter_stream.filter(track=['HurricaneHarvey', 'HurricaneHarveyRelief'])
```


**Analyzing Data**

First, I streamed in the Twitter JSON file to R using a handler to randomly sample 25% of each page of JSON lines.

```{r, results = "hide", echo=FALSE}
install.packages("jsonlite")
install.packages("rdrop2")
library(jsonlite)
library(rdrop2)

#token <- drop_auth()
saveRDS(token, "")






con_in <- gzcon(url("https://www.dropbox.com/s/khyr33yq4knr5ur/twitter_data.json?dl=0"))









con_out <- file(tmp <- tempfile(), open = "wb")
stream_in(con_in, handler= function(randsam){
  randsam <- randsam[sample(1:nrow(randsam), round(0.25*length(randsam))),]
  stream_out(randsam, con_out, pagesize=1000)
}, pagesize=500, verbose=TRUE)
close(con_out)

tweets <- stream_in(file(tmp))
nrow(tweets)
unlink(tmp)
```

420,294 records were found, which corresponds to the 420,294 tweets found during the initial stream. Then, after the random sample, 22,689 tweets were read into R.

Next I flattened the JSON file, 'tweets', just read into R to allow for analyses.

```{r, results = "hide", echo=FALSE}
tweets_flat <- flatten(tweets)
```

I created a figure to help visualize on what days the tweets were collected from, and how many tweets came from each day.

```{r, echo=FALSE}
install.packages("ggplot2")
library(ggplot2)
Sep_01_Rows <- tweets_flat[grep("Fri Sep 01", tweets_flat$created_at), ]
Sep_02_Rows <- tweets_flat[grep("Sat Sep 02", tweets_flat$created_at), ]
Sep_03_Rows <- tweets_flat[grep("Sun Sep 03", tweets_flat$created_at), ]
Sep_04_Rows <- tweets_flat[grep("Mon Sep 04", tweets_flat$created_at), ]
df.list <- list(Sep_01=Sep_01_Rows, Sep_02=Sep_02_Rows, Sep_03=Sep_03_Rows, Sep_04=Sep_04_Rows)
dat <- stack(lapply(df.list, `[[`, "created_at"))
colnames(dat)[2] <- "Day"
ggplot(dat, aes(x=Day)) + geom_bar() + labs(x = "Day of Collection", y = "Tweet Frequency", title = "Hurricane Harvey Tweets by Day Tweeted") + scale_x_discrete(labels=c("Fri 9/01", "Sat 9/02", "Sun 9/03", "Mon 9/04"))
```

Next up is to pull the coordinate data from the tweets by following a hierarchy of data collection as originally outlined in this reference (3). 

```{r, results="hide", echo=FALSE}
install.packages("tibble")
install.packages("dplyr")
library(tibble)
library(dplyr)
tweets_tbl <- as_data_frame(tweets_flat)
tweets_tbl %>% mutate(place.bounding_box.coordinates = as.character(tweets_tbl)) %>% select(place.bounding_box.coordinates)
```


The next step is to geocode my Twitter data and exclude all tweets that were made from outside the area impacted by Hurricane Harvey up through 9:56AM on 9/4/2017.

After the tweets are geocoded and restricted, I will visualize using ggplot2 in R. I will also run spatial statistics to quantify the spatial intensity and clustering of tweets. Through this method I will be able to visualize areas with a lot of Hurricane Harvey tweets coming out of them, and quantify - with statistical significance - if tweets are clustering in an area more than would be expected under complete spatial randomness. I can also compare the clustering of different areas, and quantify how much more one area may have tweet clustering as compared to another. Finally, this data can be stratified by days and times, and several maps can be created to 'track' the progress of the storm. As a final check, the Twitter heatmaps can be compared to meteorological maps for the same overlapping dates and times, and the accuracy of tracking the storm via Twitter can be assessed.


**References**
1: Moujahid, A. (2014). An Introduction to Text Mining using Twitter Streaming API and Python. http://adilmoujahid.com/posts/2014/07/twitter-analytics/. [accessed September 1, 2017].
2: Github. Tweepy: Twitter for Python! https://github.com/tweepy/tweepy. [accessed September 1, 2017].
3: Brunila, M. (2017). Scraping, extracting and mapping geodata from Twitter. http://www.mikaelbrunila.fi/2017/03/27/scraping-extracting-mapping-geodata-twitter/. [accessed September 1, 2017].


```{r devtools, echo=FALSE}
devtools::session_info()
```