---
title: "Advanced Data Science I Project"
author: "Ben Barrett"
date: "October 9, 2017"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
**Introduction**

ADD A FULL INTRODUCTION - ANY EVIDENCE OF TWEETS AND DISASTERS; USING TWEETS IN ANALYSES (OR OTHER SOCIAL MEDIA), WITH REFERENCES

These methods are designed to pull Twitter data to identify times and areas hardest hit by Hurricane Harvey. Geocoding the tweets primarily relies on the user location (entered once when a user first starts Twitter), rather than the location associated with an individual tweet, as most users turn the tweet location data off. In this case, however, these methods should be appropriate - as people who fled the impacted area before the storm hit will still have their user location linked to the impacted area. This mode of analysis relies on the assumption that the number of tweets about Hurricane Harvey coming
out of an area correlates with the level of destruction in that area.

**Data and Methods**

*Data*

The Python library 'Tweepy' was used to connect to the Twitter Streaming API and download relevant tweets. The Python program, twitter_streaming.py (reproduced below), was adapted from code provided by Mikael Brunila (1), and used to live stream tweets. The stream was set to search for the hashtags 'HurricaneHarvey' and 'HurricaneHarveyRelief', and was started at 9:10AM on 9/1/2017. The live tweet stream was stopped at 9:56AM on 9/4/2017, which resulted in a program run time of 36 hours, 46 minutes and a total of 3,289,336 KB (3.290 GB) of Twitter data collected. This corresponds to 1,491.086 KB of data per minute, on average. All of the output was saved as a JSON file, twitter_data.json, available in my Dropbox.

```{r engine='python', highlight=TRUE, eval=FALSE}
#twitter_streaming.py

#Import the necessary methods from tweepy library
import tweepy
from tweepy import Stream
from tweepy.streaming import StreamListener 
from tweepy import OAuthHandler
import json

#Variables that contains the user credentials to access Twitter API 
access_token = "Access_Token"
access_token_secret = "Access_Token_Secret"
consumer_key = "Consumer_Key"
consumer_secret = "Consumer_Secret"

auth = OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
 
api = tweepy.API(auth)
@classmethod
def parse(cls, api, raw):
    status = cls.first_parse(api, raw)
    setattr(status, 'json', json.dumps(raw))
    return status
 
# Status() is the data model for a tweet
tweepy.models.Status.first_parse = tweepy.models.Status.parse
tweepy.models.Status.parse = parse
class MyListener(StreamListener):

     def on_data(self, data):
        try:
            with open('twitter_data.json', 'a') as f:
                f.write(data)
                return True
        except BaseException as e:
            print("Error on_data: %s" % str(e))
        return True
 
     def on_error(self, status):
        print(status)
        return True
 
#Set the hashtag to be searched
twitter_stream = Stream(auth, MyListener())
twitter_stream.filter(track=['HurricaneHarvey', 'HurricaneHarveyRelief'])
```


*Methods*

Because of the large data file size, the twitter_data.json file was first streamed into R using a handler to randomly sample 25% of each page of JSON lines (total run time ~4 hours, code reproduced below). This file was then streamed back out into my Dropbox, and this reduced file serves as the basis of reproducible research.

```{r, eval=FALSE}
install.packages("jsonlite")
install.packages("rdrop2")
library(jsonlite)
library(rdrop2)

token <- drop_auth()
saveRDS(token, "~/Advanced Data Science I/AdvDataSci_Project/.gitignore.httr-oauth.RDS")
drop_auth(rdstoken= "~/Advanced Data Science I/AdvDataSci_Project/.gitignore.httr-oauth.RDS")
drop_share("twitter_data.json")

con_in <- url("https://www.dropbox.com/s/0rubxdgwvt9od66/twitter_data.json?dl=1")
con_out <- file(tmp <- tempfile(), open = "wb")
set.seed(5)
stream_in(con_in, handler= function(randsam){
  randsam <- randsam[sample(1:nrow(randsam), round(0.25*length(randsam))),]
  stream_out(randsam, con_out, pagesize=1000)
}, pagesize=500, verbose=TRUE)
close(con_out)

tweets <- stream_in(file(tmp))
nrow(tweets)
unlink(tmp)

tweets_25per <- toJSON(tweets) # GOOD
drop_upload(tweets_25per.json) # NEEDS EDITING





#### Attempt #2 - File size too big. Can try again and see if sampling readLines will work
con_in <- url("https://www.dropbox.com/s/0rubxdgwvt9od66/twitter_data.json?dl=1")
tweets <- fromJSON(sprintf("[%s%]", paste(readLines(con_in), collapse=",")))
####









```

420,294 records were found, which corresponds to the 420,294 tweets found during the initial stream. Then, after the random sample, 22,689 tweets were read into R.

Next I flattened the JSON file, 'tweets', just read into R to allow for analyses.

```{r, results = "hide", echo=FALSE}
tweets_flat <- flatten(tweets)
```

I created a figure to help visualize on what days the tweets were collected from, and how many tweets came from each day.

```{r, echo=FALSE}
install.packages("ggplot2")
library(ggplot2)
Sep_01_Rows <- tweets_flat[grep("Fri Sep 01", tweets_flat$created_at), ]
Sep_02_Rows <- tweets_flat[grep("Sat Sep 02", tweets_flat$created_at), ]
Sep_03_Rows <- tweets_flat[grep("Sun Sep 03", tweets_flat$created_at), ]
Sep_04_Rows <- tweets_flat[grep("Mon Sep 04", tweets_flat$created_at), ]
df.list <- list(Sep_01=Sep_01_Rows, Sep_02=Sep_02_Rows, Sep_03=Sep_03_Rows, Sep_04=Sep_04_Rows)
dat <- stack(lapply(df.list, `[[`, "created_at"))
colnames(dat)[2] <- "Day"
ggplot(dat, aes(x=Day)) + geom_bar() + labs(x = "Day of Collection", y = "Tweet Frequency", title = "Hurricane Harvey Tweets by Day Tweeted") + scale_x_discrete(labels=c("Fri 9/01", "Sat 9/02", "Sun 9/03", "Mon 9/04"))
```

Next up is to pull the coordinate data from the tweets by following a hierarchy of data collection as originally outlined in this reference (3). 

```{r, results="hide", echo=FALSE}
install.packages("tibble")
install.packages("dplyr")
library(tibble)
library(dplyr)
tweets_tbl <- as_data_frame(tweets_flat)
tweets_tbl %>% mutate(place.bounding_box.coordinates = as.character(tweets_tbl)) %>% select(place.bounding_box.coordinates)
```


The next step is to geocode my Twitter data and exclude all tweets that were made from outside the area impacted by Hurricane Harvey up through 9:56AM on 9/4/2017.

After the tweets are geocoded and restricted, I will visualize using ggplot2 in R. I will also run spatial statistics to quantify the spatial intensity and clustering of tweets. Through this method I will be able to visualize areas with a lot of Hurricane Harvey tweets coming out of them, and quantify - with statistical significance - if tweets are clustering in an area more than would be expected under complete spatial randomness. I can also compare the clustering of different areas, and quantify how much more one area may have tweet clustering as compared to another. Finally, this data can be stratified by days and times, and several maps can be created to 'track' the progress of the storm. As a final check, the Twitter heatmaps can be compared to meteorological maps for the same overlapping dates and times, and the accuracy of tracking the storm via Twitter can be assessed.


**References**
1: Brunila, M. (2017). Scraping, extracting and mapping geodata from Twitter. http://www.mikaelbrunila.fi/2017/03/27/scraping-extracting-mapping-geodata-twitter/. [accessed September 1, 2017].


```{r devtools, echo=FALSE}
devtools::session_info()
```