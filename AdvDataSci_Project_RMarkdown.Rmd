---
title: "Advanced Data Science I Project"
author: "Ben Barrett"
date: "September 7, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown
I will be completing the project focused on pulling social media data to identify times and areas hardest hit by Hurricane Harvey.


*Scraping Data*

I will be using Twitter as my social media platform, and will rely on a combination of the Twitter API and online tutorials to help me with scraping the data.

1) The first step is to gain access to the Twitter API, which I have done by following the tutorial found at this link: http://adilmoujahid.com/posts/2014/07/twitter-analytics/ (Note: The above link is a super useful tutorial for scraping Twitter data, and I will definitely go back to it later). Here is the link to my Twitter app page: https://apps.twitter.com/app/14193250

2) Next, I need to download and install the Python library Tweepy, in order to connect to Twitter Streaming API and download data. I have done this by following the installation instructions here (Python and pip must be installed first for this to work) (Note on pip: Must type 'python -m pip XXXX' (without quotes) in order for the command to work): https://github.com/tweepy/tweepy

3) Next, I created a Python program called twitter_streaming.py, based off of the instructions found at http://www.mikaelbrunila.fi/2017/03/27/scraping-extracting-mapping-geodata-twitter/, and have started my stream searching for the hashtags 'HurricaneHarvey' and 'HurricaneHarveyRelief'. I am exporting the output to a JSON file, twitter_data.json, which I will later have to parse for relevant data.
```{r engine='python', highlight=TRUE, eval=FALSE}
#twitter_streaming.py

#Import the necessary methods from tweepy library
import tweepy
from tweepy import Stream
from tweepy.streaming import StreamListener 
from tweepy import OAuthHandler
import json

#Variables that contains the user credentials to access Twitter API 
access_token = "Access_Token"
access_token_secret = "Access_Token_Secret"
consumer_key = "Consumer_Key"
consumer_secret = "Consumer_Secret"

auth = OAuthHandler(consumer_key, consumer_secret)
auth.set_access_token(access_token, access_token_secret)
 
api = tweepy.API(auth)
@classmethod
def parse(cls, api, raw):
    status = cls.first_parse(api, raw)
    setattr(status, 'json', json.dumps(raw))
    return status
 
# Status() is the data model for a tweet
tweepy.models.Status.first_parse = tweepy.models.Status.parse
tweepy.models.Status.parse = parse
class MyListener(StreamListener):

     def on_data(self, data):
        try:
            with open('twitter_data.json', 'a') as f:
                f.write(data)
                return True
        except BaseException as e:
            print("Error on_data: %s" % str(e))
        return True
 
     def on_error(self, status):
        print(status)
        return True
 
#Set the hashtag to be searched
twitter_stream = Stream(auth, MyListener())
twitter_stream.filter(track=['HurricaneHarvey', 'HurricaneHarveyRelief'])
```
4) I began the Twitter data stream at 9:10AM on 9/1/2017.
  a. The link http://www.mikaelbrunila.fi/2017/03/27/scraping-extracting-mapping-geodata-twitter/ referenced above also gives useful instructions for extracting geodata from tweets, and I will follow this tutorial once my data is collected.

5) Update: Stopped Twitter data stream at 9:56AM on 9/4/2017. Program run time = 36 hours, 46 minutes. A total of 3,289,336 KB (3.290 GB) of Twitter data was collected, which corresponds to 1,491.086 KB of data per minute, on average.


*Analyzing Data*

1) The next step is to geocode my Twitter data and exclude all tweets that were made from outside the area impacted by Hurricane Harvey up through 9:56AM on 9/4/2017. Geocoding the tweets will primarily rely on the user location (entered once when a user first starts Twitter), rather than the location associated with an individual tweet, as most users turn the tweet location data off. In this case, however, this should work to my advantage - as people who fled the impacted area before the storm hit should still have their user location linked to the impacted area.

2) This mode of analysis relys on the assumption that the more tweets about Hurricane Harvey coming out of an area, the worse off that area is.

3) After the tweets are geocoded and restricted, I will import them into ArcGIS for visualization.

4) I will also import the Twitter geodata into R and run spatial statistics to quantify the spatial intensity and clustering of tweets.

5) Through this method I will be able to visualize areas with a lot of Hurricane Harvey tweets coming out of them, and quantify - with statistical significance - if tweets are clustering in an area more than would be expected under complete spatial randomness. I can also compare the clustering of different areas, and quantify how much more one area may have tweet clustering as compared to another.

6) Finally, this data can be stratified by days and times, and several maps can be created to 'track' the progress of the storm. As a final check, the Twitter heatmaps can be compared to meteorological maps for the same overlapping dates and times, and the accuracy of tracking the storm via Twitter can be assessed.

```{r devtools, echo=FALSE}
devtools::session_info()
```